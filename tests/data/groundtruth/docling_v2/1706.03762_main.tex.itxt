item-0 at level 0: unspecified: group _root_
  item-1 at level 1: text: Provided proper attribution is p ... se in journalistic or scholarly works.
  item-2 at level 1: section_header: Abstract
  item-3 at level 1: text: The dominant sequence transducti ...  with large and limited training data.
  item-4 at level 1: section_header: Introduction
  item-5 at level 1: text: Recurrent neural networks, long  ... 015effective,jozefowicz2016exploring].
  item-6 at level 1: paragraph: Recurrent models typically facto ... y generate a sequence of hidden states
  item-7 at level 1: text: $h_t$, as a function of the prev ... uential computation, however, remains.
  item-8 at level 1: paragraph: Attention mechanisms have become ... tance in the input or output sequences
  item-9 at level 1: text: [bahdanau2014neural, structuredA ...  conjunction with a recurrent network.
  item-10 at level 1: paragraph: In this work we propose the Tran ... le as twelve hours on eight P100 GPUs.
  item-11 at level 1: section_header: Background
  item-12 at level 1: text: The goal of reducing sequential  ... s described in section[sec:attention].
  item-13 at level 1: paragraph: Self-attention, sometimes called ... k-independent sentence representations
  item-14 at level 1: text: [cheng2016long, decomposableAttnModel, paulus2017deep, lin2017structured].
  item-15 at level 1: paragraph: End-to-end memory networks are b ...  answering and language modeling tasks
  item-16 at level 1: text: [sukhbaatar2015].
  item-17 at level 1: paragraph: To the best of our knowledge, ho ... uss its advantages over models such as
  item-18 at level 1: text: [neural_gpu, NalBytenet2017] and [JonasFaceNet2017].
  item-19 at level 1: section_header: Model Architecture
  item-20 at level 1: section: group figure
    item-21 at level 2: picture
      item-21 at level 3: caption: Image: Figures/ModalNet-21
    item-22 at level 2: text: The Transformer - model architecture.
  item-23 at level 1: caption: Image: Figures/ModalNet-21
  item-24 at level 1: paragraph: Most competitive neural sequence ... dels have an encoder-decoder structure
  item-25 at level 1: text: [cho2014learning,bahdanau2014neu ... tional input when generating the next.
  item-26 at level 1: paragraph: The Transformer follows this ove ... in the left and right halves of Figure
  item-27 at level 1: text: [fig:model-arch], respectively.
  item-28 at level 1: section_header: Encoder and Decoder Stacks
  item-29 at level 1: text: Encoder:The encoder is composed  ... s of dimension $d_{\text{model}}=512$.
  item-30 at level 1: text: Decoder:The decoder is also comp ... wn outputs at positions less than $i$.
  item-31 at level 1: section_header: Attention
  item-32 at level 1: text: An attention function can be des ...  the query with the corresponding key.
  item-33 at level 1: section_header: Scaled Dot-Product Attention
  item-34 at level 1: paragraph: We call our particular attention "Scaled Dot-Product Attention" (Figure
  item-35 at level 1: text: [fig:multi-head-att]).   The inp ... n to obtain the weights on the values.
  item-36 at level 1: paragraph: In practice, we compute the atte ... neously, packed together into a matrix
  item-37 at level 1: text: $Q$.   The keys and values are a ...   We compute the matrix of outputs as:
  item-38 at level 1: formula: \mathrm{Attention}(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V
  item-39 at level 1: paragraph: The two most commonly used attention functions are additive attention
  item-40 at level 1: text: [bahdanau2014neural], and dot-pr ...  optimized matrix multiplication code.
  item-41 at level 1: paragraph: While for small values of
  item-42 at level 1: text: $d_k$ the two mechanisms perform ... where it has extremely small gradients
  item-43 at level 1: footnote: To illustrate why the dot produc ... k_i$, has mean $0$ and variance $d_k$.
  item-44 at level 1: text: . To counteract this effect, we  ... ot products by $\frac{1}{\sqrt{d_k}}$.
  item-45 at level 1: section_header: Multi-Head Attention
  item-46 at level 1: section: group figure
    item-47 at level 2: text: [t]0.5
    item-48 at level 2: text: Scaled Dot-Product Attention 
  0.5cm
    item-49 at level 2: picture
      item-49 at level 3: caption: Image: Figures/ModalNet-19
    item-50 at level 2: text: [t]0.5
    item-51 at level 2: text: Multi-Head Attention 
  0.1cm
    item-52 at level 2: picture
      item-52 at level 3: caption: Image: Figures/ModalNet-20
    item-53 at level 2: text: (left) Scaled Dot-Product Attent ...  attention layers running in parallel.
  item-54 at level 1: caption: Image: Figures/ModalNet-19
  item-55 at level 1: caption: Image: Figures/ModalNet-20
  item-56 at level 1: paragraph: Instead of performing a single attention function with
  item-57 at level 1: text: $d_{\text{model}}$-dimensional k ... epicted in Figure[fig:multi-head-att].
  item-58 at level 1: paragraph: Multi-head attention allows the  ... tention head, averaging inhibits this.
  item-59 at level 1: formula: \begin{align*}
    \mathrm{Multi ... QW^Q_i, KW^K_i, VW^V_i)\\
\end{align*}
  item-60 at level 1: paragraph: Where the projections are parameter matrices
  item-61 at level 1: text: $W^Q_i \in \mathbb{R}^{d_{\text{ ... bb{R}^{hd_v \times d_{\text{model}}}$.
  item-62 at level 1: paragraph: In this work we employ
  item-63 at level 1: text: $h=8$ parallel attention layers, ... ad attention with full dimensionality.
  item-64 at level 1: section_header: Applications of Attention in our Model
  item-65 at level 1: paragraph: The Transformer uses multi-head attention in three different ways:
  item-66 at level 1: list: group list
    item-67 at level 2: list_item: In "encoder-decoder attention" l ...  bahdanau2014neural,JonasFaceNet2017].
    item-68 at level 2: list_item: The encoder contains self-attent ...  in the previous layer of the encoder.
    item-69 at level 2: list_item: Similarly, self-attention layers ... ions.  See Figure[fig:multi-head-att].
  item-70 at level 1: section_header: Position-wise Feed-Forward Networks
  item-71 at level 1: paragraph: In addition to attention sub-lay ... ons with a ReLU activation in between.
  item-72 at level 1: formula: \mathrm{FFN}(x)=\max(0, xW_1 + b_1) W_2 + b_2
  item-73 at level 1: paragraph: While the linear transformations ...  dimensionality of input and output is
  item-74 at level 1: text: $d_{\text{model}}=512$, and the  ... ayer has dimensionality $d_{ff}=2048$.
  item-75 at level 1: section_header: Embeddings and Softmax
  item-76 at level 1: text: Similarly to other sequence tran ...  weights by $\sqrt{d_{\text{model}}}$.
  item-77 at level 1: section_header: Positional Encoding
  item-78 at level 1: text: Since our model contains no recu ...  learned and fixed [JonasFaceNet2017].
  item-79 at level 1: paragraph: In this work, we use sine and cosine functions of different frequencies:
  item-80 at level 1: formula: \begin{align*}
    PE_{(pos,2i)} ... 00^{2i/d_{\text{model}}})
\end{align*}
  item-81 at level 1: paragraph: where
  item-82 at level 1: text: $pos$ is the position and $i$ is ... ed as a linear function of $PE_{pos}$.
  item-83 at level 1: paragraph: We also experimented with using learned positional embeddings
  item-84 at level 1: text: [JonasFaceNet2017] instead, and  ...  the ones encountered during training.
  item-85 at level 1: section_header: Why Self-Attention
  item-86 at level 1: paragraph: In this section we compare vario ... gth sequence of symbol representations
  item-87 at level 1: text: $(x_1, ..., x_n)$ to another seq ... ttention we consider three desiderata.
  item-88 at level 1: paragraph: One is the total computational c ... ber of sequential operations required.
  item-89 at level 1: paragraph: The third is the path length bet ... it is to learn long-range dependencies
  item-90 at level 1: text: [hochreiter2001gradient]. Hence  ... composed of the different layer types.
  item-91 at level 1: text: Maximum path lengths, per-layer  ... hborhood in restricted self-attention.
  item-92 at level 1: text: -1mm
  item-93 at level 1: table with [7x4]
  item-94 at level 1: paragraph: As noted in Table
  item-95 at level 1: text: [tab:op_complexities], a self-at ...  this approach further in future work.
  item-96 at level 1: paragraph: A single convolutional layer with kernel width
  item-97 at level 1: text: $k < n$ does not connect all pai ... er, the approach we take in our model.
  item-98 at level 1: paragraph: As side benefit, self-attention  ... d semantic structure of the sentences.
  item-99 at level 1: section_header: Training
  item-100 at level 1: text: This section describes the training regime for our models.
  item-101 at level 1: section_header: Training Data and Batching
  item-102 at level 1: text: We trained on the standard WMT 2 ... source tokens and 25000 target tokens.
  item-103 at level 1: section_header: Hardware and Schedule
  item-104 at level 1: paragraph: We trained our models on one mac ... (described on the bottom line of table
  item-105 at level 1: text: [tab:variations]), step time was ...  trained for 300,000 steps (3.5 days).
  item-106 at level 1: section_header: Optimizer
  item-107 at level 1: text: We used the Adam optimizer[kingm ... of training, according to the formula:
  item-108 at level 1: formula: lrate = d_{\text{model}}^{-0.5}  ... ep\_num} \cdot {warmup\_steps}^{-1.5})
  item-109 at level 1: paragraph: This corresponds to increasing the learning rate linearly for the first
  item-110 at level 1: text: $warmup\_steps$ training steps,  ... number.  We used $warmup\_steps=4000$.
  item-111 at level 1: section_header: Regularization
  item-112 at level 1: paragraph: We employ three types of regularization during training:
  item-113 at level 1: text: Residual Dropout We apply dropou ... odel, we use a rate of $P_{drop}=0.1$.
  item-114 at level 1: text: Label Smoothing During training, ...  but improves accuracy and BLEU score.
  item-115 at level 1: section_header: Results
  item-116 at level 1: section_header: Machine Translation
  item-117 at level 1: text: The Transformer achieves better  ... ts at a fraction of the training cost.
  item-118 at level 1: text: -2mm
  item-119 at level 1: table with [13x43]
  item-120 at level 1: paragraph: On the WMT 2014 English-to-Germa ... rmer model (Transformer (big) in Table
  item-121 at level 1: text: [tab:wmt-results]) outperforms t ... cost of any of the competitive models.
  item-122 at level 1: paragraph: On the WMT 2014 English-to-Frenc ... our big model achieves a BLEU score of
  item-123 at level 1: text: $41.0$, outperforming all of the ... rate $P_{drop}=0.1$, instead of $0.3$.
  item-124 at level 1: paragraph: For the base models, we used a s ... e used beam search with a beam size of
  item-125 at level 1: text: $4$ and length penalty $\alpha=0 ... te early when possible [wu2016google].
  item-126 at level 1: paragraph: Table
  item-127 at level 1: text: [tab:wmt-results] summarizes our ... on floating-point capacity of each GPU
  item-128 at level 1: footnote: We used values of 2.8, 3.7, 6.0  ...  K80, K40, M40 and P100, respectively.
  item-129 at level 1: text: .
  item-130 at level 1: section_header: Model Variations
  item-131 at level 1: text: Variations on the Transformer ar ...  be compared to per-word perplexities.
  item-132 at level 1: text: -2mm
  item-133 at level 1: table with [23x13]
  item-134 at level 1: paragraph: To evaluate the importance of di ... ng.  We present these results in Table
  item-135 at level 1: text: [tab:variations].
  item-136 at level 1: paragraph: In Table
  item-137 at level 1: text: [tab:variations] rows (A), we va ... ty also drops off with too many heads.
  item-138 at level 1: paragraph: In Table
  item-139 at level 1: text: [tab:variations] rows (B), we ob ... y identical results to the base model.
  item-140 at level 1: section_header: English Constituency Parsing
  item-141 at level 1: text: The Transformer generalizes well ... ing (Results are on Section 23 of WSJ)
  item-142 at level 1: text: -2mm
  item-143 at level 1: table with [14x4]
  item-144 at level 1: paragraph: To evaluate if the Transformer c ... -the-art results in small-data regimes
  item-145 at level 1: text: [KVparse15].
  item-146 at level 1: paragraph: We trained a 4-layer transformer with
  item-147 at level 1: text: $d_{model} = 1024$ on the Wall S ... okens for the semi-supervised setting.
  item-148 at level 1: paragraph: We performed only a small number ... , both attention and residual (section
  item-149 at level 1: text: [sec:reg]), learning rates and b ...  only and the semi-supervised setting.
  item-150 at level 1: paragraph: Our results in Table
  item-151 at level 1: text: [tab:parsing-results] show that  ... Neural Network Grammar [dyer-rnng:16].
  item-152 at level 1: paragraph: In contrast to RNN sequence-to-sequence models
  item-153 at level 1: text: [KVparse15], the Transformer out ... the WSJ training set of 40K sentences.
  item-154 at level 1: section_header: Conclusion
  item-155 at level 1: text: In this work, we presented the T ... ures with multi-headed self-attention.
  item-156 at level 1: paragraph: For translation tasks, the Trans ... ven all previously reported ensembles.
  item-157 at level 1: paragraph: We are excited about the future  ... ial is another research goals of ours.
  item-158 at level 1: paragraph: The code we used to train and evaluate our models is available at
  item-159 at level 1: text: https://github.com/tensorflow/tensor2tensor.
  item-160 at level 1: text: AcknowledgementsWe are grateful  ... comments, corrections and inspiration.
  item-161 at level 1: text: plain
  item-162 at level 1: section_header: References
  item-163 at level 1: list: group bibliography
    item-164 at level 2: list_item: 10
    item-165 at level 2: list_item: layernorm2016
JimmyLei Ba, Jamie ... arXiv preprint arXiv:1607.06450, 2016.
    item-166 at level 2: list_item: bahdanau2014neural
Dzmitry Bahda ...  translate.
CoRR, abs/1409.0473, 2014.
    item-167 at level 2: list_item: DBLP:journals/corr/BritzGLL17
De ... itectures.
CoRR, abs/1703.03906, 2017.
    item-168 at level 2: list_item: cheng2016long
Jianpeng Cheng, Li ... arXiv preprint arXiv:1601.06733, 2016.
    item-169 at level 2: list_item: cho2014learning
Kyunghyun Cho, B ... ranslation.
CoRR, abs/1406.1078, 2014.
    item-170 at level 2: list_item: xception2016
Francois Chollet.
X ... arXiv preprint arXiv:1610.02357, 2016.
    item-171 at level 2: list_item: gruEval14
Junyoung Chung, Caglar ...   modeling.
CoRR, abs/1412.3555, 2014.
    item-172 at level 2: list_item: dyer-rnng:16
Chris Dyer, Adhigun ... ork grammars.
In Proc. of NAACL, 2016.
    item-173 at level 2: list_item: JonasFaceNet2017
Jonas Gehring,  ... Xiv preprint arXiv:1705.03122v2, 2017.
    item-174 at level 2: list_item: graves2013generating
Alex Graves ... 
arXiv preprint arXiv:1308.0850, 2013.
    item-175 at level 2: list_item: he2016deep
Kaiming He, Xiangyu Z ... ttern Recognition, pages 770778, 2016.
    item-176 at level 2: list_item: hochreiter2001gradient
Sepp Hoch ... arning long-term
  dependencies, 2001.
    item-177 at level 2: list_item: hochreiter1997
Sepp Hochreiter a ... ural computation, 9(8):17351780, 1997.
    item-178 at level 2: list_item: huang-harper:2009:EMNLP
Zhongqia ... ssing, pages 832841. ACL, August 2009.
    item-179 at level 2: list_item: jozefowicz2016exploring
Rafal Jo ... arXiv preprint arXiv:1602.02410, 2016.
    item-180 at level 2: list_item: extendedngpu
ukasz Kaiser and Sa ... on Processing Systems, (NIPS),
  2016.
    item-181 at level 2: list_item: neural_gpu
ukasz Kaiser and Ilya ... earning Representations
 (ICLR), 2016.
    item-182 at level 2: list_item: NalBytenet2017
Nal Kalchbrenner, ... Xiv preprint arXiv:1610.10099v2, 2017.
    item-183 at level 2: list_item: structuredAttentionNetworks
Yoon ... nce on Learning Representations, 2017.
    item-184 at level 2: list_item: kingma2014adam
Diederik Kingma a ... tochastic optimization.
In ICLR, 2015.
    item-185 at level 2: list_item: Kuchaiev2017Factorization
Oleksi ... arXiv preprint arXiv:1703.10722, 2017.
    item-186 at level 2: list_item: lin2017structured
Zhouhan Lin, M ... arXiv preprint arXiv:1703.03130, 2017.
    item-187 at level 2: list_item: multiseq2seq
Minh-Thang Luong, Q ... arXiv preprint arXiv:1511.06114, 2015.
    item-188 at level 2: list_item: luong2015effective
Minh-Thang Lu ... arXiv preprint arXiv:1508.04025, 2015.
    item-189 at level 2: list_item: marcus1993building
MitchellP Mar ... ional linguistics, 19(2):313330, 1993.
    item-190 at level 2: list_item: mcclosky-etAl:2006:NAACL
David M ... ference, pages 152159. ACL, June 2006.
    item-191 at level 2: list_item: decomposableAttnModel
Ankur Pari ...  in Natural Language Processing, 2016.
    item-192 at level 2: list_item: paulus2017deep
Romain Paulus, Ca ... arXiv preprint arXiv:1705.04304, 2017.
    item-193 at level 2: list_item: petrov-EtAl:2006:ACL
Slav Petrov ... e ACL, pages
  433440. ACL, July 2006.
    item-194 at level 2: list_item: press2016using
Ofir Press and Li ... arXiv preprint arXiv:1608.05859, 2016.
    item-195 at level 2: list_item: sennrich2015neural
Rico Sennrich ... arXiv preprint arXiv:1508.07909, 2015.
    item-196 at level 2: list_item: shazeer2017outrageously
Noam Sha ... arXiv preprint arXiv:1701.06538, 2017.
    item-197 at level 2: list_item: srivastava2014dropout
Nitish Sri ... arning Research, 15(1):19291958, 2014.
    item-198 at level 2: list_item: sukhbaatar2015
Sainbayar Sukhbaa ... 402448. Curran Associates, Inc., 2015.
    item-199 at level 2: list_item: sutskever14
Ilya Sutskever, Orio ... ssing Systems, pages
  31043112, 2014.
    item-200 at level 2: list_item: DBLP:journals/corr/SzegedyVISW15 ... er vision.
CoRR, abs/1512.00567, 2015.
    item-201 at level 2: list_item: KVparse15
Vinyals & Kaiser, Koo, ...  Information Processing Systems, 2015.
    item-202 at level 2: list_item: wu2016google
Yonghui Wu, Mike Sc ... arXiv preprint arXiv:1609.08144, 2016.
    item-203 at level 2: list_item: DBLP:journals/corr/ZhouCWLX16
Ji ... anslation.
CoRR, abs/1606.04199, 2016.
    item-204 at level 2: list_item: zhu-EtAl:2013:ACL
Muhua Zhu, Yue ... pers), pages 434443. ACL, August 2013.
  item-205 at level 1: section_header: Attention Visualizations
  item-206 at level 1: section: group figure
    item-207 at level 2: picture
      item-207 at level 3: caption: Image: ./vis/making_more_difficult5_new.pdf
    item-208 at level 2: text: An example of the attention mech ... different heads. Best viewed in color.
  item-209 at level 1: caption: Image: ./vis/making_more_difficult5_new.pdf
  item-210 at level 1: section: group figure
    item-211 at level 2: picture
      item-211 at level 3: caption: Image: ./vis/anaphora_resolution_new.pdf
    item-212 at level 2: picture
      item-212 at level 3: caption: Image: ./vis/anaphora_resolution2_new.pdf
    item-213 at level 2: text: Two attention heads, also in lay ... tentions are very sharp for this word.
  item-214 at level 1: caption: Image: ./vis/anaphora_resolution_new.pdf
  item-215 at level 1: caption: Image: ./vis/anaphora_resolution2_new.pdf
  item-216 at level 1: section: group figure
    item-217 at level 2: picture
      item-217 at level 3: caption: Image: ./vis/attending_to_head_new.pdf
    item-218 at level 2: picture
      item-218 at level 3: caption: Image: ./vis/attending_to_head2_new.pdf
    item-219 at level 2: text: Many of the attention heads exhi ... ly learned to perform different tasks.
  item-220 at level 1: caption: Image: ./vis/attending_to_head_new.pdf
  item-221 at level 1: caption: Image: ./vis/attending_to_head2_new.pdf