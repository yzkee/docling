%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Peter Staar   at 2022-12-09 12:43:52 +0100 


%% Saved with string encoding Unicode (UTF-8) 

@inproceedings{Desai2021TabLeXAB,
  title={TabLeX: A Benchmark Dataset for Structure and Content Information Extraction from Scientific Tables},
  author={Harsh Desai and Pratik Kayal and Mayank Kumar Singh},
  booktitle={IEEE International Conference on Document Analysis and Recognition},
  year={2021}
}

@inproceedings{GTE,
	author = {Zheng, Xinyi and Burdick, Douglas and Popa, Lucian and Zhong, Xu and Wang, Nancy Xin Ru},
	booktitle = {2021 IEEE Winter Conference on Applications of Computer Vision (WACV)},
	date-added = {2022-12-09 12:43:33 +0100},
	date-modified = {2022-12-09 12:43:33 +0100},
	doi = {10.1109/WACV48630.2021.00074},
	pages = {697-706},
	title = {Global Table Extractor (GTE): A Framework for Joint Table Identification and Cell Structure Recognition Using Visual Context},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/WACV48630.2021.00074}}

@inproceedings{KDD18,
	abstract = {Over the past few decades, the amount of scientific articles and technical literature has increased exponentially in size. Consequently, there is a great need for systems that can ingest these documents at scale and make the contained knowledge discoverable. Unfortunately, both the format of these documents (e.g. the PDF format or bitmap images) as well as the presentation of the data (e.g. complex tables) make the extraction of qualitative and quantitive data extremely challenging. In this paper, we present a modular, cloud-based platform to ingest documents at scale. This platform, called the Corpus Conversion Service (CCS), implements a pipeline which allows users to parse and annotate documents (i.e. collect ground-truth), train machine-learning classification algorithms and ultimately convert any type of PDF or bitmap-documents to a structured content representation format. We will show that each of the modules is scalable due to an asynchronous microservice architecture and can therefore handle massive amounts of documents. Furthermore, we will show that our capability to gather groundtruth is accelerated by machine-learning algorithms by at least one order of magnitude. This allows us to both gather large amounts of ground-truth in very little time and obtain very good precision/recall metrics in the range of 99\% with regard to content conversion to structured output. The CCS platform is currently deployed on IBM internal infrastructure and serving more than 250 active users for knowledge-engineering project engagements.},
	address = {New York, NY, USA},
	author = {Staar, Peter W J and Dolfi, Michele and Auer, Christoph and Bekas, Costas},
	booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
	date-added = {2022-12-09 12:41:33 +0100},
	date-modified = {2022-12-09 12:41:33 +0100},
	doi = {10.1145/3219819.3219834},
	isbn = {9781450355520},
	keywords = {ibm research, ibm, pdf, cloud architecture, deep learning, artificial intelligence, knowledge ingestion, asynchronous architecture, document conversion, table processing, ai, machine learning, cloud computing},
	location = {London, United Kingdom},
	numpages = {9},
	pages = {774--782},
	publisher = {Association for Computing Machinery},
	series = {KDD '18},
	title = {Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale},
	url = {https://doi.org/10.1145/3219819.3219834},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1145/3219819.3219834}}

@article{IEECloud22,
	author = {Christoph Auer and Michele Dolfi and Andr{\'{e}} Carvalho and Cesar Berrospi Ramis and Peter W. J. Staar},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2206-00785.bib},
	date-added = {2022-12-09 12:40:16 +0100},
	date-modified = {2022-12-09 12:40:16 +0100},
	doi = {10.48550/arXiv.2206.00785},
	eprint = {2206.00785},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Mon, 13 Jun 2022 15:31:50 +0200},
	title = {Delivering Document Conversion as a Cloud Service with High Throughput and Responsiveness},
	url = {https://doi.org/10.48550/arXiv.2206.00785},
	volume = {abs/2206.00785},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.48550/arXiv.2206.00785}}

@inproceedings{TableFormer,
	author = {Nassar, Ahmed and Livathinos, Nikolaos and Lysak, Maksym and Staar, Peter},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	date-added = {2022-12-09 12:39:11 +0100},
	date-modified = {2022-12-09 12:39:11 +0100},
	month = {June},
	pages = {4614-4623},
	title = {TableFormer: Table Structure Understanding With Transformers},
	year = {2022}}

@inproceedings{DocLayNet,
  author    = {Birgit Pfitzmann and
               Christoph Auer and
               Michele Dolfi and
               Ahmed S. Nassar and
               Peter W. J. Staar},
  editor    = {Aidong Zhang and
               Huzefa Rangwala},
  title     = {DocLayNet: {A} Large Human-Annotated Dataset for Document-Layout Segmentation},
  booktitle = {{KDD} '22: The 28th {ACM} {SIGKDD} Conference on Knowledge Discovery
               and Data Mining, Washington, DC, USA, August 14 - 18, 2022},
  pages     = {3743--3751},
  publisher = {{ACM}},
  year      = {2022},
  url       = {https://doi.org/10.1145/3534678.3539043},
  doi       = {10.1145/3534678.3539043},
  timestamp = {Mon, 15 Aug 2022 10:14:00 +0200},
  biburl    = {https://dblp.org/rec/conf/kdd/PfitzmannADNS22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{smock2022pubtables,
  title={Pub{T}ables-1{M}: Towards comprehensive table extraction from unstructured documents},
  author={Smock, Brandon and Pesala, Rohith and Abraham, Robin},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={4634-4642},
  year={2022},
  month={June}
}

@inproceedings{prasad2020cascadetabnet,
  title={CascadeTabNet: An approach for end to end table detection and structure recognition from image-based documents},
  author={Prasad, Devashish and Gadpal, Ayan and Kapadni, Kshitij and Visave, Manish and Sultanpure, Kavita},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops},
  pages={572--573},
  year={2020}
}

@inproceedings{schreiber2017deepdesrt,
  title={Deepdesrt: Deep learning for detection and structure recognition of tables in document images},
  author={Schreiber, Sebastian and Agne, Stefan and Wolf, Ivo and Dengel, Andreas and Ahmed, Sheraz},
  booktitle={2017 14th IAPR international conference on document analysis and recognition (ICDAR)},
  volume={1},
  pages={1162--1167},
  year={2017},
  organization={IEEE}
}

@INPROCEEDINGS{8978137,
  author={Siddiqui, Shoaib Ahmed and Fateh, Imran Ali and Rizvi, Syed Tahseen Raza and Dengel, Andreas and Ahmed, Sheraz},
  booktitle={2019 International Conference on Document Analysis and Recognition (ICDAR)}, 
  title={DeepTabStR: Deep Learning based Table Structure Recognition}, 
  year={2019},
  volume={},
  number={},
  pages={1403-1409},
  doi={10.1109/ICDAR.2019.00226}}

@article{chi2019complicated,
  title={Complicated table structure recognition},
  author={Chi, Zewen and Huang, Heyan and Xu, Heng-Da and Yu, Houjin and Yin, Wanxuan and Mao, Xian-Ling},
  journal={arXiv preprint arXiv:1908.04729},
  year={2019}
}

@inproceedings{xue2021tgrnet,
  title={Tgrnet: A table graph reconstruction network for table structure recognition},
  author={Xue, Wenyuan and Yu, Baosheng and Wang, Wen and Tao, Dacheng and Li, Qingyong},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1295--1304},
  year={2021}
}

@inproceedings{xue2019res2tim,
  title={ReS2TIM: Reconstruct syntactic structures from table images},
  author={Xue, Wenyuan and Li, Qingyong and Tao, Dacheng},
  booktitle={2019 International Conference on Document Analysis and Recognition (ICDAR)},
  pages={749--755},
  year={2019},
  organization={IEEE}
}

@InProceedings{PubTabNet,
author="Zhong, Xu
and ShafieiBavani, Elaheh
and Jimeno Yepes, Antonio",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="Image-Based Table Recognition: Data, Model, and Evaluation",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="564--580",
abstract="Important information that relates to a specific topic in a document is often organized in tabular format to assist readers with information retrieval and comparison, which may be difficult to provide in natural language. However, tabular data in unstructured digital documents, e.g. Portable Document Format (PDF) and images, are difficult to parse into structured machine-readable format, due to complexity and diversity in their structure and style. To facilitate image-based table recognition with deep learning, we develop and release the largest publicly available table recognition dataset PubTabNet (https://github.com/ibm-aur-nlp/PubTabNet.), containing 568k table images with corresponding structured HTML representation. PubTabNet is automatically generated by matching the XML and PDF representations of the scientific articles in PubMed Central{\texttrademark} Open Access Subset (PMCOA). We also propose a novel attention-based encoder-dual-decoder (EDD) architecture that converts images of tables into HTML code. The model has a structure decoder which reconstructs the table structure and helps the cell decoder to recognize cell content. In addition, we propose a new Tree-Edit-Distance-based Similarity (TEDS) metric for table recognition, which more appropriately captures multi-hop cell misalignment and OCR errors than the pre-established metric. The experiments demonstrate that the EDD model can accurately recognize complex tables solely relying on the image representation, outperforming the state-of-the-art by 9.7{\%} absolute TEDS score.",
isbn="978-3-030-58589-1"
}

@inproceedings{deng2019challenges,
  title={Challenges in end-to-end neural scientific table recognition},
  author={Deng, Yuntian and Rosenberg, David and Mann, Gideon},
  booktitle={2019 International Conference on Document Analysis and Recognition (ICDAR)},
  pages={894--901},
  year={2019},
  organization={IEEE}
}

@article{kayal2022tables,
  title={Tables to LaTeX: structure and content extraction from scientific tables},
  author={Kayal, Pratik and Anand, Mrinal and Desai, Harsh and Singh, Mayank},
  journal={International Journal on Document Analysis and Recognition (IJDAR)},
  pages={1--10},
  year={2022},
  publisher={Springer}
}

@InProceedings{identity_matrix,
author="Chen, Bangdong
and Peng, Dezhi
and Zhang, Jiaxin
and Ren, Yujin
and Jin, Lianwen",
editor="Porwal, Utkarsh
and Forn{\'e}s, Alicia
and Shafait, Faisal",
title="Complex Table Structure Recognition in the Wild Using Transformer and Identity Matrix-Based Augmentation",
booktitle="Frontiers in Handwriting Recognition",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="545--561",
abstract="Tables are a widely used and efficient data structure. Although people can intuitively understand table contents, it remains challenging for machines, especially the tables taken in the wild. Previous methods mainly focus on scanned or PDF tables, but ignore investigating camera-based tables. This paper treats table structure recognition (TSR) as an image-to-sequence recognition task and adopts an end-to-end trainable model for complex TSR in the wild. Specifically, the model consists of a CNN-based encoder and two Transformer-based decoding branches, which can simultaneously predict the logical and physical structures of a table. Currently available camera-based table datasets are scarce, but deep learning methods heavily rely on large-scale datasets. To alleviate data insufficiency and boost model's performance, we propose a new and effective table data augmentation method, called TabSplitter. Due to the complex structure caused by cells spanning multiple rows or columns, directly cropping will lead to damage and change the properties of these cells. To solve this problem, we proposed a matrix representation, named Identity Matrix (IM), to describe the table structure. Based on IM, we crop the tables and correct the cells whose attributes have changed, thus enhancing data diversity. Furthermore, the proposed IM facilitates the pre-processing of data and post-processing of predictions. Experimental results on several datasets demonstrate the effectiveness of the model and the TabSplitter for TSR, especially for complex tables in the wild.",
isbn="978-3-031-21648-0"
}

@InProceedings{GFTE,
author="Li, Yiren
and Huang, Zheng
and Yan, Junchi
and Zhou, Yi
and Ye, Fan
and Liu, Xianhui",
editor="Del Bimbo, Alberto
and Cucchiara, Rita
and Sclaroff, Stan
and Farinella, Giovanni Maria
and Mei, Tao
and Bertini, Marco
and Escalante, Hugo Jair
and Vezzani, Roberto",
title="GFTE: Graph-Based Financial Table Extraction",
booktitle="Pattern Recognition. ICPR International Workshops and Challenges",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="644--658",
abstract="Tabular data is a crucial form of information expression, which can organize data in a standard structure for easy information retrieval and comparison. However, in financial industry and many other fields, tables are often disclosed in unstructured digital files, e.g. Portable Document Format (PDF) and images, which are difficult to be extracted directly. In this paper, to facilitate deep learning based table extraction from unstructured digital files, we publish a standard Chinese dataset named FinTab, which contains more than 1,600 financial tables of diverse kinds and their corresponding structure representation in JSON. In addition, we propose a novel graph-based convolutional neural network model named GFTE as a baseline for future comparison. GFTE integrates image feature, position feature and textual feature together for precise edge prediction and reaches overall good results https://github.com/Irene323/GFTE.",
isbn="978-3-030-68790-8"
}

@InProceedings{maskrcnn,
author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
title = {Mask R-CNN},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
}

@InProceedings{tabstructnet,
author="Raja, Sachin
and Mondal, Ajoy
and Jawahar, C. V.",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="Table Structure Recognition Using Top-Down and Bottom-Up Cues",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="70--86",
abstract="Tables are information-rich structured objects in document images. While significant work has been done in localizing tables as graphic objects in document images, only limited attempts exist on table structure recognition. Most existing literature on structure recognition depends on extraction of meta-features from the pdf document or on the optical character recognition (ocr) models to extract low-level layout features from the image. However, these methods fail to generalize well because of the absence of meta-features or errors made by the ocr when there is a significant variance in table layouts and text organization. In our work, we focus on tables that have complex structures, dense content, and varying layouts with no dependency on meta-features and/or ocr.",
isbn="978-3-030-58604-1"
}

@inproceedings{liu2022neural,
  title={Neural Collaborative Graph Machines for Table Structure Recognition},
  author={Liu, Hao and Li, Xin and Liu, Bing and Jiang, Deqiang and Liu, Yinsong and Ren, Bo},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4533--4542},
  year={2022}
}

@InProceedings{DETR,
author="Carion, Nicolas
and Massa, Francisco
and Synnaeve, Gabriel
and Usunier, Nicolas
and Kirillov, Alexander
and Zagoruyko, Sergey",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="End-to-End Object Detection with Transformers",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="213--229",
abstract="We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster R-CNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.",
isbn="978-3-030-58452-8"
}

@inproceedings{qiao2021lgpma,
  title={Lgpma: Complicated table structure recognition with local and global pyramid mask alignment},
  author={Qiao, Liang and Li, Zaisheng and Cheng, Zhanzhan and Zhang, Peng and Pu, Shiliang and Niu, Yi and Ren, Wenqi and Tan, Wenming and Wu, Fei},
  booktitle={International Conference on Document Analysis and Recognition},
  pages={99--114},
  year={2021},
  organization={Springer}
}

@inproceedings{paliwal2019tablenet,
  title={Tablenet: Deep learning model for end-to-end table detection and tabular data extraction from scanned document images},
  author={Paliwal, Shubham Singh and Vishwanath, D and Rahul, Rohit and Sharma, Monika and Vig, Lovekesh},
  booktitle={2019 International Conference on Document Analysis and Recognition (ICDAR)},
  pages={128--133},
  year={2019},
  organization={IEEE}
}

@InProceedings{Prasad_2020_CVPR_Workshops,
author = {Prasad, Devashish and Gadpal, Ayan and Kapadni, Kshitij and Visave, Manish and Sultanpure, Kavita},
title = {CascadeTabNet: An Approach for End to End Table Detection and Structure Recognition From Image-Based Documents},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
month = {June},
year = {2020}
}

@article{pagemodelrnn,
  title={Robust PDF Document Conversion using Recurrent Neural Networks},
  volume={35},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/17777}, number={17},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  author={Livathinos, Nikolaos and Berrospi, Cesar and Lysak, Maksym and Kuropiatnyk, Viktor and Nassar, Ahmed and Carvalho, Andre and Dolfi, Michele and Auer, Christoph and Dinkla, Kasper and Staar, Peter},
  year= "2021",
  month= may,
  pages={15137-15145}
}

@inproceedings{lin2022tsrformer,
  title={TSRFormer: Table Structure Recognition with Transformers},
  author={Lin, Weihong and Sun, Zheng and Ma, Chixiang and Li, Mingze and Wang, Jiawei and Sun, Lei and Huo, Qiang},
  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
  pages={6473--6482},
  year={2022}
}

@article{ma2023robust,
  title={Robust Table Detection and Structure Recognition from Heterogeneous Document Images},
  author={Ma, Chixiang and Lin, Weihong and Sun, Lei and Huo, Qiang},
  journal={Pattern Recognition},
  volume={133},
  pages={109006},
  year={2023},
  publisher={Elsevier}
}

@misc{li2019tablebank,
    title={TableBank: A Benchmark Dataset for Table Detection and Recognition},
    author={Minghao Li and Lei Cui and Shaohan Huang and Furu Wei and Ming Zhou and Zhoujun Li},
    year={2019},
    eprint={1903.01949},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@inproceedings{PubLayNet,
  title={Publaynet: largest dataset ever for document layout analysis},
  author={Zhong, Xu and Tang, Jianbin and Yepes, Antonio Jimeno},
  booktitle={2019 International Conference on Document Analysis and Recognition (ICDAR)},
  pages={1015--1022},
  year={2019},
  organization={IEEE}
}

@misc{TableMaster,
  doi = {10.48550/ARXIV.2105.01848},
  
  url = {https://arxiv.org/abs/2105.01848},
  
  author = {Ye, Jiaquan and Qi, Xianbiao and He, Yelin and Chen, Yihao and Gu, Dengyi and Gao, Peng and Xiao, Rong},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {PingAn-VCGroup's Solution for ICDAR 2021 Competition on Scientific Literature Parsing Task B: Table Recognition to HTML},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{zhang2022split,
  title={Split, embed and merge: An accurate table structure recognizer},
  author={Zhang, Zhenrong and Zhang, Jianshu and Du, Jun and Wang, Fengren},
  journal={Pattern Recognition},
  volume={126},
  pages={108565},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{lee2022table,
  title={Table Structure Recognition Based on Grid Shape Graph},
  author={Lee, Eunji and Kwon, Junhyeong and Yang, Haeyoon and Park, Jaewoo and Lee, Soonyoung and Koo, Hyung Il and Cho, Nam Ik},
  booktitle={2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)},
  pages={1868--1873},
  year={2022},
  organization={IEEE}
}

@phdthesis{10.5555/923400,
author = {Wang, Xinxin},
advisor = {Wood, Derick},
title = {Tabular Abstraction, Editing, and Formatting},
year = {1996},
isbn = {0612093972},
publisher = {University of Waterloo},
address = {CAN},
abstract = {This dissertation investigates the composition of high-quality tables with the use of electronic tools. A generic model is designed to support the different stages of tabular composition, including the editing of logical structure, the specification of layout structure, and the formatting of concrete tables. The model separates table's logical structure from its layout structure, which consists of tabular topology and typographic style. The notion of an abstract table, which describes the logical relationships among tabular items, is formally defined and a set of logical operations is proposed to manipulate tables based on these logical relationships. An abstract table can be visualized through a layout structure specified by a set of topological rules, which determine the relative placement of tabular items in two dimensions, and a set of style rules, which determine the final appearance of different items. The absolute placement of a concrete table can be automatically generated by applying a layout specification to an abstract line. An NP-complete problem arises in the formatting process that uses automatic line breaking and determines the physical dimension of a table to satisfy user-specified size constraints. An algorithm has been designed to solve the formatting problem in polynomial time for typical tables. Based on the tabular model, a prototype tabular composition system has been implemented in a UNIX, X Windows environment. This prototype provides an interactive interface to edit the logical structure, the topology and the styles of tables. It allows us to manipulate tables based on the logical relationships tabular items, regardless of where the items are placed in the layout structure, and capable of presenting a table in different topologies and styles so that we can select a high-quality layout structure.},
note = {AAINN09397}
}
